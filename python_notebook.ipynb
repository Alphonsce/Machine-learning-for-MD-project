{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations:\n",
    "\n",
    "- выборка - тензор из картинок, таргет - вектор силы\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы используют multi output GPR, настраивая гиперпараметры $\\sigma_{cov}$ и $\\sigma_{err}$ (можно однозначно их выразить через гиперпараметры из того же sklearn: $l$ и $\\sigma$)\n",
    "\n",
    "GPR - непараметрический метод, суть в том, что мы делаем предположение о виде матрицы корреляции признаков для известных данных.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделирование в хотя бы немного более сложном случае буду писать на Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры:\n",
    "\n",
    "1)k: Количество элементов в массивах r_cut и p для каждого атома\n",
    "\n",
    "2)$r_{cut}(i)_j$, i=1..k, j=1..N: векторы r_cut для j атома тоже параметр\n",
    "\n",
    "3)$p_(i)_j$, i=1..k, j=1..N: векторы p для j атома тоже параметр\n",
    "\n",
    "4)N_neighbours for summation for IVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В GPyTorch есть имплементация многоразмерного регрессора: https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/index.html#multi-output-vector-valued-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Пока что все размерности предполагаются в системе LJ, потому что пока пытаюсь это зафитить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходно пока в coords.csv и forces.csv находятся для 2 частиц данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "from numba import jit, njit, vectorize\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy.linalg import norm as norm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''\n",
    "    \n",
    "    Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.\n",
    "\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    '''\n",
    "\n",
    "    All hyperparameters are here\n",
    "\n",
    "    '''\n",
    "    N = 2     # число атомов\n",
    "    K = 2     # можно называть это разрешением...чем число больше, тем больше размеры матрицы для атомов, фактически это число элементов в наборах p и r_cut\n",
    "\n",
    "    p = (np.random.rand(K) + 0.1).copy()\n",
    "    r_cut = (np.random.rand(K) + 0.1).copy()\n",
    "\n",
    "    N_neig= N - 1 if N != 2 else 1\n",
    "\n",
    "    # train_bs = 8\n",
    "    # val_bs = 16\n",
    "    batch_size = 8\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    f_threshold = 10    # Если сила по какой-то координате превышает это значение, то строчка исключается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеется два .csv файла:\n",
    "\n",
    "1)\n",
    "| Id(time) | 1_x | 1_y | 1_z | ... | N_z |\n",
    "|------|-----|-----|-----|-----|-----|\n",
    "|      |     |     |     |     |     |\n",
    "|      |     |     |     |     |     |\n",
    "2)\n",
    "| Id(time) | f_1_x | f_1_y | f_1_z | ... | f_N_z |\n",
    "|------|-----|-----|-----|-----|-----|\n",
    "|      |     |     |     |     |     |\n",
    "|      |     |     |     |     |     |\n",
    "\n",
    "Одна строчка отсюда превращается в N матриц (на каждый атом) с N векторами сил\n",
    "\n",
    "В идеале сделать БДху из двух сущностей: сила и координата, где полями будут их проекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0x          2.519317\n",
       "0y          2.519801\n",
       "0z        157.039283\n",
       "1x        444.125151\n",
       "1y          3.765265\n",
       "1z          2.519838\n",
       "0f_x     6300.708300\n",
       "0f_y     4599.725992\n",
       "0f_z      262.943956\n",
       "1f_x    14945.975177\n",
       "1f_y      525.511023\n",
       "1f_z     1520.725800\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_df_with_coords(coords_file_path = None, forces_file_path = None):\n",
    "    '''\n",
    "    just makes df from .csvs with coords and forces\n",
    "    '''\n",
    "    coords = pd.read_csv(coords_file_path)\n",
    "\n",
    "    forces = pd.read_csv(forces_file_path)\n",
    "\n",
    "    if CFG.N != int(coords.columns[-1][:-1]) + 1:\n",
    "        raise Exception('Constant N is not equal to amount of particles in .csv')\n",
    "\n",
    "    return pd.merge(left=coords, right=forces, on='t').drop('t', axis='columns')\n",
    "\n",
    "df = create_df_with_coords('coords.csv', 'forces.csv')\n",
    "df\n",
    "\n",
    "df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0x</th>\n",
       "      <th>0y</th>\n",
       "      <th>0z</th>\n",
       "      <th>1x</th>\n",
       "      <th>1y</th>\n",
       "      <th>1z</th>\n",
       "      <th>0f_x</th>\n",
       "      <th>0f_y</th>\n",
       "      <th>0f_z</th>\n",
       "      <th>1f_x</th>\n",
       "      <th>1f_y</th>\n",
       "      <th>1f_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.047038</td>\n",
       "      <td>1.473031</td>\n",
       "      <td>1.044249</td>\n",
       "      <td>0.209507</td>\n",
       "      <td>1.060259</td>\n",
       "      <td>1.461480</td>\n",
       "      <td>12.563132</td>\n",
       "      <td>6.191671</td>\n",
       "      <td>-6.258551</td>\n",
       "      <td>-12.563132</td>\n",
       "      <td>-6.191671</td>\n",
       "      <td>6.258551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.043931</td>\n",
       "      <td>1.471776</td>\n",
       "      <td>1.055985</td>\n",
       "      <td>0.212613</td>\n",
       "      <td>1.061513</td>\n",
       "      <td>1.449744</td>\n",
       "      <td>17.251242</td>\n",
       "      <td>8.513640</td>\n",
       "      <td>-8.171147</td>\n",
       "      <td>-17.251242</td>\n",
       "      <td>-8.513640</td>\n",
       "      <td>8.171147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.041184</td>\n",
       "      <td>1.470706</td>\n",
       "      <td>1.067817</td>\n",
       "      <td>0.215360</td>\n",
       "      <td>1.062584</td>\n",
       "      <td>1.437912</td>\n",
       "      <td>22.890348</td>\n",
       "      <td>11.312408</td>\n",
       "      <td>-10.258358</td>\n",
       "      <td>-22.890348</td>\n",
       "      <td>-11.312408</td>\n",
       "      <td>10.258358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.038933</td>\n",
       "      <td>1.469890</td>\n",
       "      <td>1.079763</td>\n",
       "      <td>0.217611</td>\n",
       "      <td>1.063400</td>\n",
       "      <td>1.425966</td>\n",
       "      <td>29.352905</td>\n",
       "      <td>14.527388</td>\n",
       "      <td>-12.372807</td>\n",
       "      <td>-29.352905</td>\n",
       "      <td>-14.527388</td>\n",
       "      <td>12.372807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.037348</td>\n",
       "      <td>1.469414</td>\n",
       "      <td>1.091828</td>\n",
       "      <td>0.219196</td>\n",
       "      <td>1.063876</td>\n",
       "      <td>1.413902</td>\n",
       "      <td>36.256710</td>\n",
       "      <td>17.971601</td>\n",
       "      <td>-14.272840</td>\n",
       "      <td>-36.256710</td>\n",
       "      <td>-17.971601</td>\n",
       "      <td>14.272840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18652</th>\n",
       "      <td>0.318435</td>\n",
       "      <td>1.602216</td>\n",
       "      <td>1.093433</td>\n",
       "      <td>0.938109</td>\n",
       "      <td>0.931074</td>\n",
       "      <td>1.412297</td>\n",
       "      <td>-27.854084</td>\n",
       "      <td>30.167584</td>\n",
       "      <td>-14.332804</td>\n",
       "      <td>27.854084</td>\n",
       "      <td>-30.167584</td>\n",
       "      <td>14.332804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18653</th>\n",
       "      <td>0.319275</td>\n",
       "      <td>1.614207</td>\n",
       "      <td>1.090613</td>\n",
       "      <td>0.937269</td>\n",
       "      <td>0.919082</td>\n",
       "      <td>1.415117</td>\n",
       "      <td>-19.877967</td>\n",
       "      <td>22.358935</td>\n",
       "      <td>-10.437791</td>\n",
       "      <td>19.877967</td>\n",
       "      <td>-22.358935</td>\n",
       "      <td>10.437791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18654</th>\n",
       "      <td>0.319586</td>\n",
       "      <td>1.626171</td>\n",
       "      <td>1.087671</td>\n",
       "      <td>0.936958</td>\n",
       "      <td>0.907118</td>\n",
       "      <td>1.418058</td>\n",
       "      <td>-13.765046</td>\n",
       "      <td>16.032146</td>\n",
       "      <td>-7.366366</td>\n",
       "      <td>13.765046</td>\n",
       "      <td>-16.032146</td>\n",
       "      <td>7.366366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18655</th>\n",
       "      <td>0.319544</td>\n",
       "      <td>1.638099</td>\n",
       "      <td>1.084654</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.895191</td>\n",
       "      <td>1.421075</td>\n",
       "      <td>-9.220831</td>\n",
       "      <td>11.094284</td>\n",
       "      <td>-5.023973</td>\n",
       "      <td>9.220831</td>\n",
       "      <td>-11.094284</td>\n",
       "      <td>5.023973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18730</th>\n",
       "      <td>0.305160</td>\n",
       "      <td>0.007871</td>\n",
       "      <td>0.856847</td>\n",
       "      <td>0.951384</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>1.648882</td>\n",
       "      <td>-9.795066</td>\n",
       "      <td>0.034770</td>\n",
       "      <td>-12.005185</td>\n",
       "      <td>9.795066</td>\n",
       "      <td>-0.034770</td>\n",
       "      <td>12.005185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0x        0y        0z        1x        1y        1z       0f_x  \\\n",
       "161    1.047038  1.473031  1.044249  0.209507  1.060259  1.461480  12.563132   \n",
       "162    1.043931  1.471776  1.055985  0.212613  1.061513  1.449744  17.251242   \n",
       "163    1.041184  1.470706  1.067817  0.215360  1.062584  1.437912  22.890348   \n",
       "164    1.038933  1.469890  1.079763  0.217611  1.063400  1.425966  29.352905   \n",
       "165    1.037348  1.469414  1.091828  0.219196  1.063876  1.413902  36.256710   \n",
       "...         ...       ...       ...       ...       ...       ...        ...   \n",
       "18652  0.318435  1.602216  1.093433  0.938109  0.931074  1.412297 -27.854084   \n",
       "18653  0.319275  1.614207  1.090613  0.937269  0.919082  1.415117 -19.877967   \n",
       "18654  0.319586  1.626171  1.087671  0.936958  0.907118  1.418058 -13.765046   \n",
       "18655  0.319544  1.638099  1.084654  0.937000  0.895191  1.421075  -9.220831   \n",
       "18730  0.305160  0.007871  0.856847  0.951384  0.005577  1.648882  -9.795066   \n",
       "\n",
       "            0f_y       0f_z       1f_x       1f_y       1f_z  \n",
       "161     6.191671  -6.258551 -12.563132  -6.191671   6.258551  \n",
       "162     8.513640  -8.171147 -17.251242  -8.513640   8.171147  \n",
       "163    11.312408 -10.258358 -22.890348 -11.312408  10.258358  \n",
       "164    14.527388 -12.372807 -29.352905 -14.527388  12.372807  \n",
       "165    17.971601 -14.272840 -36.256710 -17.971601  14.272840  \n",
       "...          ...        ...        ...        ...        ...  \n",
       "18652  30.167584 -14.332804  27.854084 -30.167584  14.332804  \n",
       "18653  22.358935 -10.437791  19.877967 -22.358935  10.437791  \n",
       "18654  16.032146  -7.366366  13.765046 -16.032146   7.366366  \n",
       "18655  11.094284  -5.023973   9.220831 -11.094284   5.023973  \n",
       "18730   0.034770 -12.005185   9.795066  -0.034770  12.005185  \n",
       "\n",
       "[278 rows x 12 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for numb in range(CFG.N):\n",
    "    for coord in ['x', 'y', 'z']:\n",
    "        \n",
    "\n",
    "df.loc[df[abs(df['1f_z']) > 5].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 индекс - 1 отн 2\n",
    "\n",
    "$$\n",
    "\\vec{r_1} = \\vec{r_2} + \\vec{r}_{12}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{r}_{12} = \\vec{r_1} - \\vec{r}_{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_relative_positions(row, atom_number):\n",
    "    '''\n",
    "    This function processes one row of csv into something that we can work with\n",
    "\n",
    "    Returns np.array matrix that consists of relative positions vectors for passed atom_number to every other atom\n",
    "    and then we can chose only closest N_neighbours in the next functions\n",
    "    \n",
    "    row: df.iloc[row] - typeof(row): pd.Series\n",
    "    \n",
    "    returns: Rel_matrix, f_vec\n",
    "    '''\n",
    "\n",
    "    s_coord = pd.Series(dtype=float)\n",
    "    other_atom_numbers = [i for i in range(CFG.N) if i != atom_number]\n",
    "\n",
    "    for other_numb in other_atom_numbers:\n",
    "        index = str(atom_number) + str(other_numb)\n",
    "        for axis in ['x', 'y', 'z']:\n",
    "            s_coord[index + axis] = row[str(atom_number) + axis] - row[str(other_numb) + axis]\n",
    "\n",
    "    # we need force vector only for atom_number:\n",
    "    force_vec = []\n",
    "    for f_axis in ['f_x', 'f_y', 'f_z']:\n",
    "        force_vec.append(row[str(atom_number) + f_axis])\n",
    "        \n",
    "\n",
    "    Rel_matrix = []\n",
    "    cur_vector = []\n",
    "\n",
    "    for (i, elem) in enumerate(s_coord.values):\n",
    "        if i % 3 == 0 and i != 0:\n",
    "            Rel_matrix.append(cur_vector)\n",
    "            cur_vector = []\n",
    "\n",
    "        cur_vector.append(elem)\n",
    "    Rel_matrix.append(cur_vector)\n",
    "\n",
    "    return np.array(Rel_matrix), np.array(force_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.61478041,  0.82656999,  0.84915872]]),\n",
       " array([ 0.94574503, -1.2715507 , -1.30629998]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_relative_positions(df.iloc[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(fastmath=True)\n",
    "def make_one_vec_transformed(vec, vec_norm, r_cut_i, p_i):\n",
    "    '''\n",
    "    vec: np.array - normalized vector\n",
    "    norm: its norm\n",
    "    r_cut_i: i-th component of\n",
    "    '''\n",
    "    return vec * np.exp(\n",
    "        -np.power((vec_norm / r_cut_i), p_i)\n",
    "        )\n",
    "\n",
    "make_matrix_transformed = np.vectorize(make_one_vec_transformed)\n",
    "\n",
    "def create_V_i(i, normalized_m, norms, r_cut=CFG.r_cut, p=CFG.p):\n",
    "    '''\n",
    "    normalized_m: matrix of relative distances, where rows - normalized vectors\n",
    "    i: i-th component of r_cut and p, i in range 1..K (or in 0..K-1 in code)\n",
    "    '''\n",
    "    transf_vecs = make_matrix_transformed(normalized_m, norms[:, np.newaxis], r_cut[i], p[i])\n",
    "\n",
    "    return np.sum(transf_vecs, axis=0)\n",
    "\n",
    "# @njit(parallel=True)\n",
    "def create_V(normalized_m, norms, K=CFG.K):\n",
    "    '''\n",
    "    creates V\n",
    "    '''\n",
    "    V = []\n",
    "    for i in range(K):\n",
    "        V.append(\n",
    "            create_V_i(i, normalized_m, norms)\n",
    "        )\n",
    "\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @njit(\n",
    "#     # parallel=True,\n",
    "#     # fastmath=True\n",
    "#     )\n",
    "def _calculate_matrix_for_atom(relative_distances, r_cut=CFG.r_cut, p=CFG.p, N_neig=CFG.N_neig, K=CFG.K):\n",
    "    '''\n",
    "\n",
    "    relative_distances: np.array matrix of relative distance vectors\n",
    "\n",
    "    '''\n",
    "\n",
    "    norms = norm(relative_distances, axis=-1)\n",
    "    \n",
    "    # Only closest N_neig are counting:\n",
    "    indexlist = np.argsort(norm(relative_distances, axis=1))\n",
    "    relative_distances = relative_distances[indexlist[len(relative_distances) - N_neig:]]\n",
    "\n",
    "    normalized_rel_distances = relative_distances / norms[:, np.newaxis]\n",
    "\n",
    "    # print(\n",
    "    #     create_V_i(0, normalized_rel_distances, norms), f'{CFG.r_cut=}, {CFG.p=}'\n",
    "    # )\n",
    "\n",
    "    V = create_V(normalized_rel_distances, norms)\n",
    "    \n",
    "    A = V / norm(V, axis=-1)[:, np.newaxis]\n",
    "\n",
    "    X = V @ A.T\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_for_atom(row = None, atom_number = None):\n",
    "    '''\n",
    "\n",
    "    This function will create X matrix for passed atom with\n",
    "    arrays of r_cut and p of length k\n",
    "\n",
    "    It is a wrapper for _get_relative_positions and _calculate_matrix_for_atom, so I can speed up matrix calculations\n",
    "    with numba for _calculate_matrix_for_atom\n",
    "\n",
    "    atom_number: a number of atom that we are passing\n",
    "    row: one row from df_with_coords, i.e. df.iloc[index_of_row]\n",
    "\n",
    "    '''\n",
    "\n",
    "    # creating row of relative coordinates for concrete atom:\n",
    "    relative_distances, f_vec = _get_relative_positions(row=row, atom_number=atom_number)\n",
    "    X = _calculate_matrix_for_atom(relative_distances=relative_distances)\n",
    "    \n",
    "    return X, f_vec\n",
    "\n",
    "# %timeit get_matrix_for_atom(row=df.iloc[0], atom_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**У нас будет train и val выборки, все-таки выборку, для который известен таргет принято называть validation, на которой мы качество оцениваем, а test это все-таки выборка, для который неизвестны таргеты**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df: pd.DataFrame):\n",
    "    '''\n",
    "\n",
    "    Убирает строчки с аномально большими силами из df\n",
    "\n",
    "    '''\n",
    "    threshold = CFG.f_threshold\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_dataset(coords_file_path = 'coords.csv', forces_file_path = 'forces.csv', step=1, transform=None):\n",
    "    '''\n",
    "\n",
    "    Примитивная версия датасета, просто все будет хранить в одном тензоре...\n",
    "\n",
    "    Эта функция - wrapper на все выше написанные функции, она по переданным путям к .csv\n",
    "    возвращает тензор из матриц для каждого атома в каждой строчке и тензор из векторов сил\n",
    "\n",
    "\n",
    "    ИНогда есть смысл делать побольше шаг между соседними строчками, поскольку если есть почти одинаковые матрицы, то\n",
    "    это по-сути линейная зависимость и модель тогда надо сильнее регулизировать\n",
    "\n",
    "    transform: преобразование к X части датасета, в основном для нормализации нужно\n",
    "    step: через сколько строчек шагать при чтении csv в датасет, чтобы уж совсем одинаковых не было\n",
    "\n",
    "    '''\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    df = create_df_with_coords(coords_file_path=coords_file_path, forces_file_path=forces_file_path)#.loc[range(1000), :]\n",
    "\n",
    "    # Сюда будет вставлена чистка df от больших сил\n",
    "    df = clean_df(df)\n",
    "\n",
    "    row_indexes = [i for i in range(0, len(df.index), step)]\n",
    "\n",
    "    for atom_number in range(CFG.N):\n",
    "        for index in tqdm(row_indexes, desc=f'Progress for atom {atom_number}'):\n",
    "            row = df.iloc[index]\n",
    "            x, f = get_matrix_for_atom(row=row, atom_number=atom_number)\n",
    "\n",
    "            if transform:\n",
    "                x = transform(x)\n",
    "            else:\n",
    "                x = transforms.ToTensor()(x)\n",
    "            x = x.to(torch.float)\n",
    "\n",
    "            dataset.append(\n",
    "                (x, torch.tensor(f, dtype=torch.float))\n",
    "                )\n",
    "            \n",
    "            # В дальнейшем для других моделей может иметь смысл хранить и возвращать тут (x, f, A), где A - соответствующая матрица для X\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Когда только начинаю работать с датасетом надо один раз на трейне посчитать std и mean, чтобы нормализовать можно было\n",
    "\n",
    "mean = 0.14509494602680206\n",
    "std = 0.09590677171945572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# все-таки у нас тут не картинки будут, поэтому я попробую сначала даже без нормализации, нормализовать надо 1 канал, если в терминах картинки рассуждать\n",
    "\n",
    "transform = transforms.Compose([                                    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),                    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress for atom 0:   3%|▎         | 476/19000 [00:00<00:27, 672.49it/s]/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: invalid value encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "/tmp/ipykernel_91737/4279244184.py:26: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  A = V / norm(V, axis=-1)[:, np.newaxis]\n",
      "Progress for atom 0:  60%|█████▉    | 11375/19000 [00:17<00:11, 667.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '01y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py:1085\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1083'>1084</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1084'>1085</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_with_engine(key, value)\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1085'>1086</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py:1146\u001b[0m, in \u001b[0;36mSeries._set_with_engine\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1144'>1145</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_with_engine\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1145'>1146</a>\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1147'>1148</a>\u001b[0m     \u001b[39m# this is equivalent to self._values[key] = value\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: '01y'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000027?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m create_tensor_dataset(\u001b[39m'\u001b[39;49m\u001b[39mcoords.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mforces.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, step\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, transform\u001b[39m=\u001b[39;49mtransform)\n",
      "\u001b[1;32m/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb Cell 25'\u001b[0m in \u001b[0;36mcreate_tensor_dataset\u001b[0;34m(coords_file_path, forces_file_path, step, transform)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000024?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m tqdm(row_indexes, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mProgress for atom \u001b[39m\u001b[39m{\u001b[39;00matom_number\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000024?line=24'>25</a>\u001b[0m     row \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[index]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000024?line=25'>26</a>\u001b[0m     x, f \u001b[39m=\u001b[39m get_matrix_for_atom(row\u001b[39m=\u001b[39;49mrow, atom_number\u001b[39m=\u001b[39;49matom_number)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000024?line=27'>28</a>\u001b[0m     \u001b[39mif\u001b[39;00m transform:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000024?line=28'>29</a>\u001b[0m         x \u001b[39m=\u001b[39m transform(x)\n",
      "\u001b[1;32m/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb Cell 22'\u001b[0m in \u001b[0;36mget_matrix_for_atom\u001b[0;34m(row, atom_number)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000021?line=1'>2</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000021?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000021?line=3'>4</a>\u001b[0m \u001b[39mThis function will create X matrix for passed atom with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000021?line=11'>12</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000021?line=12'>13</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000021?line=14'>15</a>\u001b[0m \u001b[39m# creating row of relative coordinates for concrete atom:\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000021?line=15'>16</a>\u001b[0m relative_distances, f_vec \u001b[39m=\u001b[39m _get_relative_positions(row\u001b[39m=\u001b[39;49mrow, atom_number\u001b[39m=\u001b[39;49matom_number)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000021?line=16'>17</a>\u001b[0m X \u001b[39m=\u001b[39m _calculate_matrix_for_atom(relative_distances\u001b[39m=\u001b[39mrelative_distances)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000021?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m X, f_vec\n",
      "\u001b[1;32m/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb Cell 17'\u001b[0m in \u001b[0;36m_get_relative_positions\u001b[0;34m(row, atom_number)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000016?line=16'>17</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(atom_number) \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(other_numb)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000016?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m axis \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mz\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000016?line=18'>19</a>\u001b[0m         s_coord[index \u001b[39m+\u001b[39m axis] \u001b[39m=\u001b[39m row[\u001b[39mstr\u001b[39m(atom_number) \u001b[39m+\u001b[39m axis] \u001b[39m-\u001b[39m row[\u001b[39mstr\u001b[39m(other_numb) \u001b[39m+\u001b[39m axis]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000016?line=20'>21</a>\u001b[0m \u001b[39m# we need force vector only for atom_number:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000016?line=21'>22</a>\u001b[0m force_vec \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py:1104\u001b[0m, in \u001b[0;36mSeries.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1100'>1101</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39msetitem_inplace(key, value)\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1101'>1102</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1102'>1103</a>\u001b[0m         \u001b[39m# GH#12862 adding a new key to the Series\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1103'>1104</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1105'>1106</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (InvalidIndexError, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1106'>1107</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/series.py?line=1107'>1108</a>\u001b[0m         \u001b[39m# cases with MultiIndex don't get here bc they raise KeyError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:716\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=712'>713</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=714'>715</a>\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[0;32m--> <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=715'>716</a>\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:1682\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1678'>1679</a>\u001b[0m     indexer, missing \u001b[39m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1680'>1681</a>\u001b[0m     \u001b[39mif\u001b[39;00m missing:\n\u001b[0;32m-> <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1681'>1682</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer_missing(indexer, value)\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1682'>1683</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1684'>1685</a>\u001b[0m \u001b[39m# align and set the values\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py:1951\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1948'>1949</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1949'>1950</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39mindex\n\u001b[0;32m-> <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1950'>1951</a>\u001b[0m     new_index \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49minsert(\u001b[39mlen\u001b[39;49m(index), indexer)\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1952'>1953</a>\u001b[0m     \u001b[39m# we have a coerced indexer, e.g. a float\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1953'>1954</a>\u001b[0m     \u001b[39m# that matches in an Int64Index, so\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1954'>1955</a>\u001b[0m     \u001b[39m# we will not create a duplicate index, rather\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1955'>1956</a>\u001b[0m     \u001b[39m# index to that element\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1956'>1957</a>\u001b[0m     \u001b[39m# e.g. 0.0 -> 0\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1957'>1958</a>\u001b[0m     \u001b[39m# GH#12246\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1958'>1959</a>\u001b[0m     \u001b[39mif\u001b[39;00m index\u001b[39m.\u001b[39mis_unique:\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1959'>1960</a>\u001b[0m         \u001b[39m# pass new_index[-1:] instead if [new_index[-1]]\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexing.py?line=1960'>1961</a>\u001b[0m         \u001b[39m#  so that we retain dtype\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py:6613\u001b[0m, in \u001b[0;36mIndex.insert\u001b[0;34m(self, loc, item)\u001b[0m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=6608'>6609</a>\u001b[0m     new_values[loc] \u001b[39m=\u001b[39m item\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=6610'>6611</a>\u001b[0m \u001b[39m# Use self._constructor instead of Index to retain NumericIndex GH#43921\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=6611'>6612</a>\u001b[0m \u001b[39m# TODO(2.0) can use Index instead of self._constructor\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=6612'>6613</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_constructor\u001b[39m.\u001b[39;49m_with_infer(new_values, name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py:680\u001b[0m, in \u001b[0;36mIndex._with_infer\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=677'>678</a>\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=678'>679</a>\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m.*the Index constructor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=679'>680</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=681'>682</a>\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m _dtype_obj \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39m_is_multi:\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=682'>683</a>\u001b[0m     \u001b[39m# error: Argument 1 to \"maybe_convert_objects\" has incompatible type\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=683'>684</a>\u001b[0m     \u001b[39m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=684'>685</a>\u001b[0m     \u001b[39m# \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=685'>686</a>\u001b[0m     values \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmaybe_convert_objects(result\u001b[39m.\u001b[39m_values)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py:411\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=401'>402</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=402'>403</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPassing keywords other than \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcopy\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=403'>404</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtupleize_cols\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is deprecated and will raise TypeError in a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=406'>407</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=407'>408</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=409'>410</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m \u001b[39mimport\u001b[39;00m PandasArray\n\u001b[0;32m--> <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=410'>411</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mindexes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrange\u001b[39;00m \u001b[39mimport\u001b[39;00m RangeIndex\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=412'>413</a>\u001b[0m name \u001b[39m=\u001b[39m maybe_extract_name(name, data, \u001b[39mcls\u001b[39m)\n\u001b[1;32m    <a href='file:///home/alphonse/anaconda3/envs/myenv/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=414'>415</a>\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = create_tensor_dataset('coords.csv', 'forces.csv', step=1, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока никакие параметры особо не надо настраивать, поэтому и кросс валидацию не буду делать пока что, затем ее можно сделать, передавая в функцию create_dataloaders еще один параметр - фолд, на котором трейн, предварительно поделив на фолды датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если просто брать в качестве трейна другие строчки из одной генерации, то можно не отследить переобучения, стоит пробовать тестить на датасете, который отдельно сгенерирован с таким же числом частиц, который модель еще вообще не видела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1457,  0.1457],\n",
       "          [-1.4108, -1.4108]]]),\n",
       " tensor([-0.0040,  0.0084,  0.0057]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.33, random_state=42)\n",
    "\n",
    "train_data[62]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Код для выяснения mean и std у трейновой выборки: (по ненормализованному датасету делается)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = nan, std = nan\n"
     ]
    }
   ],
   "source": [
    "def get_mean_and_std_for_train_X(train_data):\n",
    "    train_X = torch.cat([row[0] for row in train_data])\n",
    "\n",
    "    print(\n",
    "        f'mean = {torch.mean(train_X)}, std = {torch.std(train_X)}'\n",
    "    )\n",
    "\n",
    "get_mean_and_std_for_train_X(train_data=train_data) # тупо проверка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Когда молекул уже будет много как хранить данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта клетка нужна будет, когда молекул будет много (N > 100, K порядка 100)\n",
    "\n",
    "def create_df_with_paths(df_coords: pd.DataFrame, first_folder = 'Atom_matrices'):\n",
    "    '''\n",
    "\n",
    "    Пока эта функция не нужна, но в будущем за счет нее как раз будет работать PathBasedDataset\n",
    "\n",
    "    gets df, returns df with paths to torch matrices for each atom for different times,\n",
    "    basically this function will call get_matrix_for_atom a lot of times\n",
    "\n",
    "    output: pd.DataFrame that orignated from this:\n",
    "    \n",
    "    | Index | 1_atom_X_path                     | ... | N_atom_X_path                     |\n",
    "    |-------|-----------------------------------|-----|-----------------------------------|\n",
    "    | 1     | ./atom_matrices/index1/atom1.tb   |     | ./atom_matrices/index1/atomN.tb   |\n",
    "    | ...   |                                   |     |                                   |\n",
    "    | 30k   | ./atom_matrices/index30k/atom1.tb |     | ./atom_matrices/index30k/atomN.tb |\n",
    "    \n",
    "    but eventually will look like this:\n",
    "\n",
    "    | Index   | atom_X_path                       |\n",
    "    |---------|-----------------------------------|\n",
    "    | 1       | ./atom_matrices/index1/atom1.tb   |\n",
    "    | ...     | ...                               |\n",
    "    | 30k * N | ./atom_matrices/index30k/atomN.tb |\n",
    "\n",
    "    '''\n",
    "    row_numbers = df_coords.index\n",
    "\n",
    "    df_paths = pd.DataFrame(\n",
    "        {\n",
    "            'path': []\n",
    "        }\n",
    "    )\n",
    "\n",
    "    pass\n",
    "\n",
    "class PathBasedDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "\n",
    "    Это будет класс датасета из торча для большого числа молекул, если молекул будет очень много, то надо будет уже хранить все матрицы X не в оперативной памяти\n",
    "\n",
    "    При создании экземпляра будет передаваться pd.Dataframe, который\n",
    "    состоит из трех колонок - проекций вектора силы и еще одной колонки - путь к файлу, где лежит как-то заэнкоженная\n",
    "    матрица для данного атома, и так для каждого атома (я проверил, что запись и чтение при помощи torch.save и torch.load для тензоров очень быстрое)\n",
    "\n",
    "    '''\n",
    "    def __init__(self, df, transforms=None, mode='train'):\n",
    "        self.df = df    # it will be dataframe with coordinates and forces of all atoms\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = 1   # it will be a matrix KxK for each atom\n",
    "        y = 1   # it will be a force vector with shape: (3)\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dataset, val_dataset, train_bs=CFG.batch_size, val_bs=CFG.batch_size, fold=None):\n",
    "    '''\n",
    "\n",
    "    Returns train_loader, val_loader\n",
    "\n",
    "    fold: will be used in cross validation, when I will implement it\n",
    "\n",
    "    '''\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=train_bs, shuffle=True)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=val_bs, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = create_dataloaders(train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Обучение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я попробую оба варианта:\n",
    "1) Многомерный аутпут\n",
    "2) Для каждой компоненты свой одномерный аутпут\n",
    "\n",
    "надо помнить, что сначала на двух частицах!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Многомерный аутпут:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще постоянный множитель - это не особо важно, но просто при оценке качества модели возникнут определенные трудности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultuOutputMSELoss(nn.MSELoss):\n",
    "    '''\n",
    "\n",
    "    Custom loss that calculates average over batch loss for multidim MSE - sum of MSE for components\n",
    "\n",
    "    Example:\n",
    "    |\n",
    "    |    Loss = MultuOutputMSELoss()\n",
    "    |\n",
    "    |    a = torch.ones((8, 3))      # it is batch of 8 vectors of size 3\n",
    "    |    b = torch.zeros((8, 3))\n",
    "    |\n",
    "    |    Loss(a, b, batch_size=8) -> 3\n",
    "\n",
    "    '''\n",
    "\n",
    "    def forward(self, input, target, batch_size=CFG.batch_size):\n",
    "        '''\n",
    "        оно при reduction='mean' делит на произведение всех размерностей\n",
    "        '''\n",
    "        return F.mse_loss(input, target, reduction='sum') / batch_size   # или эквивалентно делать reduction='mean' и умножать на input.size()[-1] - length of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10., dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = MultuOutputMSELoss()\n",
    "\n",
    "a = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3]\n",
    "])\n",
    "b = torch.tensor([\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1]\n",
    "], dtype=torch.double)\n",
    "\n",
    "L(a, b, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNet(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Когда будет много частиц, здесь можно свертки и пуллинги запихать, в FC сетку уже надо подавать распрямленную матрицу\n",
    "        # self.conv1 = ...\n",
    "\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(CFG.K * CFG.K, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(1024),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - is batch of matrices KxK\n",
    "\n",
    "        # Здесь происходят какие-то там свертки, пуллинги и тп...\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fully_connected(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метрики буду использовать сумму MSE по компонентам, лоссы попробую разные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer, scheduler, scaler=None):\n",
    "    '''\n",
    "\n",
    "    Функция обучения по всем батчам 1 раз (1 эпоха)\n",
    "\n",
    "    scaler: gradient scaler from torch.amp, попозже добавлю обучение с ним\n",
    "\n",
    "    '''\n",
    "    running_loss = 0.0\n",
    "    running_MSE = 0\n",
    "    processed_data = 0\n",
    "  \n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        inputs = inputs.to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_MSE += F.mse_loss(input=outputs, target=labels, reduction='sum').item()\n",
    "        processed_data += inputs.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "              \n",
    "    train_loss = running_loss / processed_data\n",
    "    train_MSE = running_MSE / processed_data\n",
    "    \n",
    "    return train_loss, train_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_loader, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_MSE = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_MSE += F.mse_loss(input=outputs, target=labels, reduction='sum')\n",
    "        processed_size += inputs.size(0)\n",
    "\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_MSE = running_MSE.double() / processed_size\n",
    "    \n",
    "    return val_loss, val_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, optimizer, scheduler, epochs, scaler=None, criterion=MultuOutputMSELoss()):\n",
    "    '''\n",
    "\n",
    "    Полный цикл обучения\n",
    "    \n",
    "    '''\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_acc {t_mse:0.4f} val_acc {v_mse:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_MSE = fit_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "            print(\"loss\", train_loss)\n",
    "\n",
    "            val_loss, val_MSE = eval_epoch(model, val_loader, criterion)\n",
    "            if epoch != 0:\n",
    "                if history[-1][-1] < val_MSE:\n",
    "                    torch.save(model.state_dict(), './model.pth')     # сохраняем модель напрямую в гугл диск \n",
    "            \n",
    "            history.append((train_loss, train_MSE, val_loss, val_MSE))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch + 1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_mse=train_MSE, v_mse=val_MSE))\n",
    "            \n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleNet(output_size=3).to(CFG.device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=7e-3, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "# scheduler.step нужно первый раз делать обязательно после optimizer.step, потому что иначе мы просто пропустим первый шаг scheduler\n",
    "exp_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попытка для двух частиц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 154.77891783723658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  10%|█         | 1/10 [00:04<00:39,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 154.7789     val_loss 139.9067 train_acc 154.7793 val_acc 140.0669\n",
      "loss 185.592015986937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|██        | 2/10 [00:08<00:32,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 002 train_loss: 185.5920     val_loss 136.3207 train_acc 185.5924 val_acc 136.4849\n",
      "loss 388.31427190308483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  30%|███       | 3/10 [00:11<00:27,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 003 train_loss: 388.3143     val_loss 136.7962 train_acc 388.3146 val_acc 136.9619\n",
      "loss 152.34939445894906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|████      | 4/10 [00:15<00:22,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 004 train_loss: 152.3494     val_loss 136.7341 train_acc 152.3499 val_acc 136.8969\n",
      "loss 150.2734109047727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  50%|█████     | 5/10 [00:19<00:18,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 005 train_loss: 150.2734     val_loss 137.4509 train_acc 150.6563 val_acc 137.6170\n",
      "loss 149.92218087873212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|██████    | 6/10 [00:22<00:14,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 006 train_loss: 149.9222     val_loss 136.3228 train_acc 149.9224 val_acc 136.4850\n",
      "loss 149.88849653991676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  70%|███████   | 7/10 [00:26<00:11,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 007 train_loss: 149.8885     val_loss 136.3360 train_acc 149.8889 val_acc 136.4978\n",
      "loss 149.46445904257536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|████████  | 8/10 [00:30<00:07,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 008 train_loss: 149.4645     val_loss 136.3212 train_acc 149.9463 val_acc 136.4843\n",
      "loss 149.9332806679178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  90%|█████████ | 9/10 [00:33<00:03,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 009 train_loss: 149.9333     val_loss 136.3332 train_acc 149.9336 val_acc 136.4969\n",
      "loss 149.86301196442753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [00:37<00:00,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 010 train_loss: 149.8630     val_loss 136.2958 train_acc 149.8633 val_acc 136.4583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = train(\n",
    "    train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer,\n",
    "    scheduler=exp_scheduler, epochs=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1f3fbe8920da735554002e2c4451c18264ac8836341d5bf958934d377042186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
