{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations:\n",
    "\n",
    "- выборка - тензор из картинок, таргет - вектор силы\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Авторы используют multi output GPR, настраивая гиперпараметры $\\sigma_{cov}$ и $\\sigma_{err}$ (можно однозначно их выразить через гиперпараметры из того же sklearn: $l$ и $\\sigma$)\n",
    "\n",
    "GPR - непараметрический метод, суть в том, что мы делаем предположение о виде матрицы корреляции признаков для известных данных.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделирование в хотя бы немного более сложном случае буду писать на Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры:\n",
    "\n",
    "1)k: Количество элементов в массивах r_cut и p для каждого атома\n",
    "\n",
    "2)$r_{cut}(i)_j$, i=1..k, j=1..N: векторы r_cut для j атома тоже параметр\n",
    "\n",
    "3)$p_(i)_j$, i=1..k, j=1..N: векторы p для j атома тоже параметр\n",
    "\n",
    "4)N_neighbours for summation for IVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В GPyTorch есть имплементация многоразмерного регрессора: https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/index.html#multi-output-vector-valued-functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Пока что все размерности предполагаются в системе LJ, потому что пока пытаюсь это зафитить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходно пока в coords.csv и forces.csv находятся для 2 частиц данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "from numba import jit, njit, vectorize\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy.linalg import norm as norm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''\n",
    "    \n",
    "    Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.\n",
    "\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    '''\n",
    "\n",
    "    All hyperparameters are here\n",
    "\n",
    "    '''\n",
    "    N = 2     # число атомов\n",
    "    K = 2     # можно называть это разрешением...чем число больше, тем больше размеры матрицы для атомов, фактически это число элементов в наборах p и r_cut\n",
    "\n",
    "    p = (np.random.rand(K) + 0.1).copy()\n",
    "    r_cut = (np.random.rand(K) + 0.1).copy()\n",
    "\n",
    "    N_neig= N - 1 if N != 2 else 1\n",
    "\n",
    "    # train_bs = 8\n",
    "    # val_bs = 16\n",
    "    batch_size = 8\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеется два .csv файла:\n",
    "\n",
    "1)\n",
    "| Id(time) | 1_x | 1_y | 1_z | ... | N_z |\n",
    "|------|-----|-----|-----|-----|-----|\n",
    "|      |     |     |     |     |     |\n",
    "|      |     |     |     |     |     |\n",
    "2)\n",
    "| Id(time) | f_1_x | f_1_y | f_1_z | ... | f_N_z |\n",
    "|------|-----|-----|-----|-----|-----|\n",
    "|      |     |     |     |     |     |\n",
    "|      |     |     |     |     |     |\n",
    "\n",
    "Одна строчка отсюда превращается в N матриц (на каждый атом) с N векторами сил\n",
    "\n",
    "В идеале сделать БДху из двух сущностей: сила и координата, где полями будут их проекции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_with_coords(coords_file_path = None, forces_file_path = None):\n",
    "    '''\n",
    "    just makes df from .csvs with coords and forces\n",
    "    '''\n",
    "    coords = pd.read_csv(coords_file_path)\n",
    "\n",
    "    forces = pd.read_csv(forces_file_path)\n",
    "\n",
    "    if CFG.N != int(coords.columns[-1][:-1]) + 1:\n",
    "        raise Exception('Constant N is not equal to amount of particles in .csv')\n",
    "\n",
    "    return pd.merge(left=coords, right=forces, on='t').drop('t', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 индекс - 1 отн 2\n",
    "\n",
    "$$\n",
    "\\vec{r_1} = \\vec{r_2} + \\vec{r}_{12}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{r}_{12} = \\vec{r_1} - \\vec{r}_{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_relative_positions(row, atom_number):\n",
    "    '''\n",
    "    This function processes one row of csv into something that we can work with\n",
    "\n",
    "    Returns np.array matrix that consists of relative positions vectors for passed atom_number to every other atom\n",
    "    and then we can chose only closest N_neighbours in the next functions\n",
    "    \n",
    "    row: df.iloc[row] - typeof(row): pd.Series\n",
    "    \n",
    "    returns: Rel_matrix, f_vec\n",
    "    '''\n",
    "\n",
    "    s_coord = pd.Series(dtype=float)\n",
    "    other_atom_numbers = [i for i in range(CFG.N) if i != atom_number]\n",
    "\n",
    "    for other_numb in other_atom_numbers:\n",
    "        index = str(atom_number) + str(other_numb)\n",
    "        for axis in ['x', 'y', 'z']:\n",
    "            s_coord[index + axis] = row[str(atom_number) + axis] - row[str(other_numb) + axis]\n",
    "\n",
    "    # we need force vector only for atom_number:\n",
    "    force_vec = []\n",
    "    for f_axis in ['f_x', 'f_y', 'f_z']:\n",
    "        force_vec.append(row[str(atom_number) + f_axis])\n",
    "        \n",
    "\n",
    "    Rel_matrix = []\n",
    "    cur_vector = []\n",
    "\n",
    "    for (i, elem) in enumerate(s_coord.values):\n",
    "        if i % 3 == 0 and i != 0:\n",
    "            Rel_matrix.append(cur_vector)\n",
    "            cur_vector = []\n",
    "\n",
    "        cur_vector.append(elem)\n",
    "    Rel_matrix.append(cur_vector)\n",
    "\n",
    "    return np.array(Rel_matrix), np.array(force_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(fastmath=True)\n",
    "def make_one_vec_transformed(vec, vec_norm, r_cut_i, p_i):\n",
    "    '''\n",
    "    vec: np.array - normalized vector\n",
    "    norm: its norm\n",
    "    r_cut_i: i-th component of\n",
    "    '''\n",
    "    return vec * np.exp(\n",
    "        -np.power((vec_norm / r_cut_i), p_i)\n",
    "        )\n",
    "\n",
    "make_matrix_transformed = np.vectorize(make_one_vec_transformed)\n",
    "\n",
    "def create_V_i(i, normalized_m, norms, r_cut=CFG.r_cut, p=CFG.p):\n",
    "    '''\n",
    "    normalized_m: matrix of relative distances, where rows - normalized vectors\n",
    "    i: i-th component of r_cut and p, i in range 1..K (or in 0..K-1 in code)\n",
    "    '''\n",
    "    transf_vecs = make_matrix_transformed(normalized_m, norms[:, np.newaxis], r_cut[i], p[i])\n",
    "\n",
    "    return np.sum(transf_vecs, axis=0)\n",
    "\n",
    "# @njit(parallel=True)\n",
    "def create_V(normalized_m, norms, K=CFG.K):\n",
    "    '''\n",
    "    creates V\n",
    "    '''\n",
    "    V = []\n",
    "    for i in range(K):\n",
    "        V.append(\n",
    "            create_V_i(i, normalized_m, norms)\n",
    "        )\n",
    "\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @njit(\n",
    "#     # parallel=True,\n",
    "#     # fastmath=True\n",
    "#     )\n",
    "def _calculate_matrix_for_atom(relative_distances, r_cut=CFG.r_cut, p=CFG.p, N_neig=CFG.N_neig, K=CFG.K):\n",
    "    '''\n",
    "\n",
    "    relative_distances: np.array matrix of relative distance vectors\n",
    "\n",
    "    '''\n",
    "\n",
    "    norms = norm(relative_distances, axis=-1)\n",
    "    \n",
    "    # Only closest N_neig are counting:\n",
    "    indexlist = np.argsort(norm(relative_distances, axis=1))\n",
    "    relative_distances = relative_distances[indexlist[len(relative_distances) - N_neig:]]\n",
    "\n",
    "    normalized_rel_distances = relative_distances / norms[:, np.newaxis]\n",
    "\n",
    "    # print(\n",
    "    #     create_V_i(0, normalized_rel_distances, norms), f'{CFG.r_cut=}, {CFG.p=}'\n",
    "    # )\n",
    "\n",
    "    V = create_V(normalized_rel_distances, norms)\n",
    "    \n",
    "    A = V / norm(V, axis=-1)[:, np.newaxis]\n",
    "\n",
    "    X = V @ A.T\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_for_atom(row = None, atom_number = None):\n",
    "    '''\n",
    "\n",
    "    This function will create X matrix for passed atom with\n",
    "    arrays of r_cut and p of length k\n",
    "\n",
    "    It is a wrapper for _get_relative_positions and _calculate_matrix_for_atom, so I can speed up matrix calculations\n",
    "    with numba for _calculate_matrix_for_atom\n",
    "\n",
    "    atom_number: a number of atom that we are passing\n",
    "    row: one row from df_with_coords, i.e. df.iloc[index_of_row]\n",
    "\n",
    "    '''\n",
    "\n",
    "    # creating row of relative coordinates for concrete atom:\n",
    "    relative_distances, f_vec = _get_relative_positions(row=row, atom_number=atom_number)\n",
    "    X = _calculate_matrix_for_atom(relative_distances=relative_distances)\n",
    "    \n",
    "    return X, f_vec\n",
    "\n",
    "# %timeit get_matrix_for_atom(row=df.iloc[0], atom_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**У нас будет train и val выборки, все-таки выборку, для который известен таргет принято называть validation, на которой мы качество оцениваем, а test это все-таки выборка, для который неизвестны таргеты**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_dataset(coords_file_path = 'coords.csv', forces_file_path = 'forces.csv', step=1, transform=None):\n",
    "    '''\n",
    "\n",
    "    Примитивная версия датасета, просто все будет хранить в одном тензоре...\n",
    "\n",
    "    Эта функция - wrapper на все выше написанные функции, она по переданным путям к .csv\n",
    "    возвращает тензор из матриц для каждого атома в каждой строчке и тензор из векторов сил\n",
    "\n",
    "\n",
    "    ИНогда есть смысл делать побольше шаг между соседними строчками, поскольку если есть почти одинаковые матрицы, то\n",
    "    это по-сути линейная зависимость и модель тогда надо сильнее регулизировать\n",
    "\n",
    "    transform: преобразование к X части датасета, в основном для нормализации нужно\n",
    "    step: через сколько строчек шагать при чтении csv в датасет, чтобы уж совсем одинаковых не было\n",
    "\n",
    "    '''\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    df = create_df_with_coords(coords_file_path=coords_file_path, forces_file_path=forces_file_path)#.loc[range(1000), :]\n",
    "    row_indexes = [i for i in range(0, len(df.index), step)]\n",
    "\n",
    "    for atom_number in range(CFG.N):\n",
    "        for index in tqdm(row_indexes, desc=f'Progress for atom {atom_number}'):\n",
    "            row = df.iloc[index]\n",
    "            x, f = get_matrix_for_atom(row=row, atom_number=atom_number)\n",
    "\n",
    "            if transform:\n",
    "                x = transform(x)\n",
    "            else:\n",
    "                x = transforms.ToTensor()(x)\n",
    "            x = x.to(torch.float)\n",
    "\n",
    "            dataset.append(\n",
    "                (x, torch.tensor(f, dtype=torch.float))\n",
    "                )\n",
    "            \n",
    "            # В дальнейшем для других моделей может иметь смысл хранить и возвращать тут (x, f, A), где A - соответствующая матрица для X\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Когда только начинаю работать с датасетом надо один раз на трейне посчитать std и mean, чтобы нормализовать можно было\n",
    "\n",
    "mean = 0.31259544571596093\n",
    "std = 0.1928839687413107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# все-таки у нас тут не картинки будут, поэтому я попробую сначала даже без нормализации, нормализовать надо 1 канал, если в терминах картинки рассуждать\n",
    "\n",
    "transform = transforms.Compose([                                    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std),                    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress for atom 0: 100%|██████████| 9500/9500 [00:13<00:00, 697.89it/s]\n",
      "Progress for atom 1: 100%|██████████| 9500/9500 [00:13<00:00, 696.95it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = create_tensor_dataset('coords.csv', 'forces.csv', step=10, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока никакие параметры особо не надо настраивать, поэтому и кросс валидацию не буду делать пока что, затем ее можно сделать, передавая в функцию create_dataloaders еще один параметр - фолд, на котором трейн, предварительно поделив на фолды датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если просто брать в качестве трейна другие строчки из одной генерации, то можно не отследить переобучения, стоит пробовать тестить на датасете, который отдельно сгенерирован с таким же числом частиц, который модель еще вообще не видела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb Cell 30'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000029?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000029?line=2'>3</a>\u001b[0m train_data, val_data \u001b[39m=\u001b[39m train_test_split(dataset, test_size\u001b[39m=\u001b[39m\u001b[39m0.33\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alphonse/Machine-learning-for-MD-project/python_notebook.ipynb#ch0000029?line=4'>5</a>\u001b[0m train_data[\u001b[39m100000\u001b[39;49m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.33, random_state=42)\n",
    "\n",
    "train_data[10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Код для выяснения mean и std у трейновой выборки: (по ненормализованному датасету делается)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 1.0655578641660668e-08, std = 1.0\n"
     ]
    }
   ],
   "source": [
    "def get_mean_and_std_for_train_X(train_data):\n",
    "    train_X = torch.cat([row[0] for row in train_data])\n",
    "\n",
    "    print(\n",
    "        f'mean = {torch.mean(train_X)}, std = {torch.std(train_X)}'\n",
    "    )\n",
    "\n",
    "get_mean_and_std_for_train_X(train_data=train_data) # тупо проверка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Когда молекул уже будет много как хранить данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта клетка нужна будет, когда молекул будет много (N > 100, K порядка 100)\n",
    "\n",
    "def create_df_with_paths(df_coords: pd.DataFrame, first_folder = 'Atom_matrices'):\n",
    "    '''\n",
    "\n",
    "    Пока эта функция не нужна, но в будущем за счет нее как раз будет работать PathBasedDataset\n",
    "\n",
    "    gets df, returns df with paths to torch matrices for each atom for different times,\n",
    "    basically this function will call get_matrix_for_atom a lot of times\n",
    "\n",
    "    output: pd.DataFrame that orignated from this:\n",
    "    \n",
    "    | Index | 1_atom_X_path                     | ... | N_atom_X_path                     |\n",
    "    |-------|-----------------------------------|-----|-----------------------------------|\n",
    "    | 1     | ./atom_matrices/index1/atom1.tb   |     | ./atom_matrices/index1/atomN.tb   |\n",
    "    | ...   |                                   |     |                                   |\n",
    "    | 30k   | ./atom_matrices/index30k/atom1.tb |     | ./atom_matrices/index30k/atomN.tb |\n",
    "    \n",
    "    but eventually will look like this:\n",
    "\n",
    "    | Index   | atom_X_path                       |\n",
    "    |---------|-----------------------------------|\n",
    "    | 1       | ./atom_matrices/index1/atom1.tb   |\n",
    "    | ...     | ...                               |\n",
    "    | 30k * N | ./atom_matrices/index30k/atomN.tb |\n",
    "\n",
    "    '''\n",
    "    row_numbers = df_coords.index\n",
    "\n",
    "    df_paths = pd.DataFrame(\n",
    "        {\n",
    "            'path': []\n",
    "        }\n",
    "    )\n",
    "\n",
    "    pass\n",
    "\n",
    "class PathBasedDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "\n",
    "    Это будет класс датасета из торча для большого числа молекул, если молекул будет очень много, то надо будет уже хранить все матрицы X не в оперативной памяти\n",
    "\n",
    "    При создании экземпляра будет передаваться pd.Dataframe, который\n",
    "    состоит из трех колонок - проекций вектора силы и еще одной колонки - путь к файлу, где лежит как-то заэнкоженная\n",
    "    матрица для данного атома, и так для каждого атома (я проверил, что запись и чтение при помощи torch.save и torch.load для тензоров очень быстрое)\n",
    "\n",
    "    '''\n",
    "    def __init__(self, df, transforms=None, mode='train'):\n",
    "        self.df = df    # it will be dataframe with coordinates and forces of all atoms\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = 1   # it will be a matrix KxK for each atom\n",
    "        y = 1   # it will be a force vector with shape: (3)\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dataset, val_dataset, train_bs=CFG.batch_size, val_bs=CFG.batch_size, fold=None):\n",
    "    '''\n",
    "\n",
    "    Returns train_loader, val_loader\n",
    "\n",
    "    fold: will be used in cross validation, when I will implement it\n",
    "\n",
    "    '''\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=train_bs, shuffle=True)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=val_bs, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = create_dataloaders(train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Обучение:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я попробую оба варианта:\n",
    "1) Многомерный аутпут\n",
    "2) Для каждой компоненты свой одномерный аутпут\n",
    "\n",
    "надо помнить, что сначала на двух частицах!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)Многомерный аутпут:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще постоянный множитель - это не особо важно, но просто при оценке качества модели возникнут определенные трудности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultuOutputMSELoss(nn.MSELoss):\n",
    "    '''\n",
    "\n",
    "    Custom loss that calculates average over batch loss for multidim MSE - sum of MSE for components\n",
    "\n",
    "    Example:\n",
    "    |\n",
    "    |    Loss = MultuOutputMSELoss()\n",
    "    |\n",
    "    |    a = torch.ones((8, 3))      # it is batch of 8 vectors of size 3\n",
    "    |    b = torch.zeros((8, 3))\n",
    "    |\n",
    "    |    Loss(a, b, batch_size=8) -> 3\n",
    "\n",
    "    '''\n",
    "\n",
    "    def forward(self, input, target, batch_size=CFG.batch_size):\n",
    "        '''\n",
    "        оно при reduction='mean' делит на произведение всех размерностей\n",
    "        '''\n",
    "        return F.mse_loss(input, target, reduction='sum') / batch_size   # или эквивалентно делать reduction='mean' и умножать на input.size()[-1] - length of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10., dtype=torch.float64)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = MultuOutputMSELoss()\n",
    "\n",
    "a = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3]\n",
    "])\n",
    "b = torch.tensor([\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 0, 1]\n",
    "], dtype=torch.double)\n",
    "\n",
    "L(a, b, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNet(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Когда будет много частиц, здесь можно свертки и пуллинги запихать, в FC сетку уже надо подавать распрямленную матрицу\n",
    "        # self.conv1 = ...\n",
    "\n",
    "        self.fully_connected = nn.Sequential(\n",
    "            nn.Linear(CFG.K * CFG.K, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(1024),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - is batch of matrices KxK\n",
    "\n",
    "        # Здесь происходят какие-то там свертки, пуллинги и тп...\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fully_connected(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метрики буду использовать сумму MSE по компонентам, лоссы попробую разные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer, scheduler, scaler=None):\n",
    "    '''\n",
    "\n",
    "    Функция обучения по всем батчам 1 раз (1 эпоха)\n",
    "\n",
    "    scaler: gradient scaler from torch.amp, попозже добавлю обучение с ним\n",
    "\n",
    "    '''\n",
    "    running_loss = 0.0\n",
    "    running_MSE = 0\n",
    "    processed_data = 0\n",
    "  \n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        inputs = inputs.to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_MSE += F.mse_loss(input=outputs, target=labels, reduction='sum').item()\n",
    "        processed_data += inputs.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "              \n",
    "    train_loss = running_loss / processed_data\n",
    "    train_MSE = running_MSE / processed_data\n",
    "    \n",
    "    return train_loss, train_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_loader, criterion):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_MSE = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_MSE += F.mse_loss(input=outputs, target=labels, reduction='sum')\n",
    "        processed_size += inputs.size(0)\n",
    "\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_MSE = running_MSE.double() / processed_size\n",
    "    \n",
    "    return val_loss, val_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, optimizer, scheduler, epochs, scaler=None, criterion=MultuOutputMSELoss()):\n",
    "    '''\n",
    "\n",
    "    Полный цикл обучения\n",
    "    \n",
    "    '''\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_acc {t_mse:0.4f} val_acc {v_mse:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_MSE = fit_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "            print(\"loss\", train_loss)\n",
    "\n",
    "            val_loss, val_MSE = eval_epoch(model, val_loader, criterion)\n",
    "            if epoch != 0:\n",
    "                if history[-1][-1] < val_MSE:\n",
    "                    torch.save(model.state_dict(), './model.pth')     # сохраняем модель напрямую в гугл диск \n",
    "            \n",
    "            history.append((train_loss, train_MSE, val_loss, val_MSE))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch + 1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_mse=train_MSE, v_mse=val_MSE))\n",
    "            \n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SingleNet(output_size=3).to(CFG.device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-3, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "# scheduler.step нужно первый раз делать обязательно после optimizer.step, потому что иначе мы просто пропустим первый шаг scheduler\n",
    "exp_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попытка для двух частиц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  10%|█         | 1/10 [00:01<00:14,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 136.49288998535891\n",
      "\n",
      "Epoch 001 train_loss: 136.4929     val_loss 137.7162 train_acc 136.4929 val_acc 137.7189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|██        | 2/10 [00:03<00:12,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 135.1319561550737\n",
      "\n",
      "Epoch 002 train_loss: 135.1320     val_loss 127.9772 train_acc 135.1320 val_acc 127.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  30%|███       | 3/10 [00:04<00:11,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 131.22261174064775\n",
      "\n",
      "Epoch 003 train_loss: 131.2226     val_loss 128.3886 train_acc 131.2226 val_acc 128.3914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|████      | 4/10 [00:06<00:09,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 130.80750325488646\n",
      "\n",
      "Epoch 004 train_loss: 130.8075     val_loss 127.4418 train_acc 130.8075 val_acc 127.4444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  50%|█████     | 5/10 [00:07<00:07,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 129.83282623510786\n",
      "\n",
      "Epoch 005 train_loss: 129.8328     val_loss 126.4848 train_acc 129.8328 val_acc 126.4909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|██████    | 6/10 [00:09<00:06,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 126.9525360941221\n",
      "\n",
      "Epoch 006 train_loss: 126.9525     val_loss 125.0821 train_acc 126.9525 val_acc 125.0858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  70%|███████   | 7/10 [00:11<00:04,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 125.9217702423394\n",
      "\n",
      "Epoch 007 train_loss: 125.9218     val_loss 124.5196 train_acc 125.9218 val_acc 124.5236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|████████  | 8/10 [00:12<00:03,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 125.40312170582777\n",
      "\n",
      "Epoch 008 train_loss: 125.4031     val_loss 124.3471 train_acc 125.4031 val_acc 124.3507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  90%|█████████ | 9/10 [00:14<00:01,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 124.85093026654015\n",
      "\n",
      "Epoch 009 train_loss: 124.8509     val_loss 123.9617 train_acc 124.8509 val_acc 123.9653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [00:15<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 124.38468920718358\n",
      "\n",
      "Epoch 010 train_loss: 124.3847     val_loss 123.2818 train_acc 124.3847 val_acc 123.2859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = train(\n",
    "    train_loader=train_loader, val_loader=val_loader, model=model, optimizer=optimizer,\n",
    "    scheduler=exp_scheduler, epochs=10\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1f3fbe8920da735554002e2c4451c18264ac8836341d5bf958934d377042186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
