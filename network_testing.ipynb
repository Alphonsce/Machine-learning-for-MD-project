{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создал этот ноутбук для проверки простеньких сеток на синтетических датасетах / кусках датасетов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Векторы V все почти что коллинеарны, когда их много - стоит уже посмотреть на $p_i$ и $r_{cut_{i}}$\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(X):\n",
    "    return 5 * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '3_dataset_K_3.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    '''\n",
    "\n",
    "    All hyperparameters are here\n",
    "\n",
    "    '''\n",
    "\n",
    "    N = int(path.split('_')[0])     # число атомов\n",
    "    K = int(path.split('_')[-1].split('.')[0])     # можно называть это разрешением...чем число больше, тем больше размеры матрицы для атомов, фактически это число элементов в наборах p и r_cut\n",
    "\n",
    "    L = 2 * N ** (1 / 3) # размер одной клетки при моделировании\n",
    "\n",
    "    r_cut = np.random.uniform(low=5, high=10, size=K).copy()\n",
    "    p = np.random.uniform(low=1, high=3, size=K).copy()\n",
    "    N_neig= N - 1 if N != 2 else 1\n",
    "\n",
    "    # train_bs = 8\n",
    "    # val_bs = 16\n",
    "    batch_size = 64\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    f_threshold = 5    # Если сила по какой-то координате превышает это значение, то строчка исключается, совсем маленьких по модулю сил быть не должно, если что при генерации просто r_cut поменьше надо делать\n",
    "    coord_threshold = L     # Если вдруг очень большие расстояния, то надо выкидывать\n",
    "    f_min_threshold = 0.05\n",
    "    #\n",
    "    output_size = K     # Размерность аутпута модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dataset, val_dataset, train_bs=64, val_bs=64, fold=None):\n",
    "    '''\n",
    "\n",
    "    Returns train_loader, val_loader\n",
    "\n",
    "    fold: will be used in cross validation, when I will implement it\n",
    "\n",
    "    '''\n",
    "    \n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=train_bs, shuffle=True)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset, batch_size=val_bs, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_dim_f_norm_from_x_norm(data_sample, figsize=(15, 10), fontsize=18):\n",
    "    '''\n",
    "\n",
    "    Функция строит зависимость таргета от X\n",
    "\n",
    "    data_sample - итерируемый объект из тьюплов вида: (x, f_k_dim, f_3d, pinv_A)\n",
    "\n",
    "    '''\n",
    "    plt.figure(figsize=figsize)\n",
    "    x = torch.stack([elem[0][0].squeeze() for elem in data_sample])\n",
    "\n",
    "    y = torch.stack([elem[1][0].squeeze() for elem in data_sample])\n",
    "\n",
    "    plt.scatter(x, y, label='y(x)')\n",
    "    plt.title('Зависимость $Y_{target}(X)$', fontsize=fontsize)\n",
    "\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiOutputMSELoss(nn.MSELoss):\n",
    "#     '''\n",
    "\n",
    "#     Custom loss that calculates average over batch loss for multidim MSE - sum of MSE for components\n",
    "\n",
    "#     Example:\n",
    "#     |\n",
    "#     |    Loss = MultiOutputMSELoss()\n",
    "#     |\n",
    "#     |    a = torch.ones((8, 3))      # it is batch of 8 vectors of size 3\n",
    "#     |    b = torch.zeros((8, 3))\n",
    "#     |\n",
    "#     |    Loss(a, b, batch_size=8) -> 3\n",
    "\n",
    "#     '''\n",
    "\n",
    "#     def forward(self, input, target, batch_size=CFG.batch_size):\n",
    "#         '''\n",
    "#         оно при reduction='mean' делит на произведение всех размерностей\n",
    "#         '''\n",
    "#         # при очень большом размере батча последние батчи будут например размера 128 вместо 256, поэтому просто умножать на батч сайз неправильно, могут быть другого размера\n",
    "\n",
    "#         return F.mse_loss(input, target, reduction='sum') / input.size(0)   # или эквивалентно делать reduction='mean' и умножать на input.size()[-1] - length of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_and_plot(model, X_matrices=None, Y_target=None, figsize=(30, 20), fontsize=20, criterion=nn.MSELoss(), data=None, same_axis=False):\n",
    "    '''\n",
    "\n",
    "    Строит предсказанную и тагрет зависимости\n",
    "\n",
    "    Можно подавать либо отлельно X_matrices, Y_target либо подать датасет из тьюплов: (X, f_k_dim, f_3d, A_pinv)\n",
    "\n",
    "    Будет подаваться 3 итерируемых объекта: набор(батч или кусок датасета) из \"матриц\", предсказания, таргеты\n",
    "\n",
    "    Качество\n",
    "\n",
    "    '''\n",
    "\n",
    "    if data:\n",
    "        X_matrices, Y_target = list(map(lambda x: torch.stack(x), list(zip(*data))))\n",
    "        \n",
    "    Y_pred = model(X_matrices)\n",
    "    metric = criterion(Y_pred, Y_target)\n",
    "    names = ['predicted', 'target']\n",
    "    Ys = [Y_pred, Y_target]\n",
    "    metric = criterion(Y_pred, Y_target)\n",
    "    \n",
    "    if CFG.K != 1:\n",
    "        print(\n",
    "            f'Metric: {metric}'\n",
    "        )\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    fig = plt.gcf()\n",
    "    fig.suptitle(f'Metric: {metric}', fontsize=22)\n",
    "\n",
    "    for (i, name) in enumerate(names):\n",
    "        if not same_axis:\n",
    "            plt.subplot(1, 2, i + 1)\n",
    "\n",
    "        plt.scatter(X_matrices.squeeze().detach().numpy(), Ys[i].squeeze().detach().numpy(), label='Y(X)')\n",
    "\n",
    "        plt.xlabel('X', fontsize=18)\n",
    "        plt.ylabel('Y', fontsize=18)\n",
    "        plt.legend(loc='best', fontsize=fontsize)\n",
    "        plt.title(f'{name}', fontsize=18)\n",
    "        plt.grid(alpha=0.4)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer, scheduler=None, scaler=None):\n",
    "    '''\n",
    "\n",
    "    Функция обучения по всем батчам 1 раз (1 эпоха)\n",
    "\n",
    "    scaler: gradient scaler from torch.amp, попозже добавлю обучение с ним\n",
    "\n",
    "    В данной версии: (X, f_k_dim)\n",
    "\n",
    "    Лосс выводится для k-мерного предсказания, а mse считается по 3D вариантам, однако при K=1 3d и 1d MSE совпадают\n",
    "\n",
    "    '''\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # running_MSE = 0\n",
    "    processed_data = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = inputs.to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)    # при очень большом размере батча последние два батча будут например размера 128 вместо 256, поэтому просто умножать на батч сайз неправильно, могут быть другого размера\n",
    "        processed_data += inputs.size(0)\n",
    "\n",
    "    # print(labels)\n",
    "    train_loss = running_loss / processed_data\n",
    "    # train_MSE = running_MSE / processed_data\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, val_loader, criterion):\n",
    "    '''\n",
    "\n",
    "    Одна эпоха по val выборке\n",
    "\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    # running_MSE = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "\n",
    "        inputs = inputs.to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        processed_size += inputs.size(0)\n",
    "\n",
    "    # print(f' outputs:\\n{outputs}, \\n labels: \\n {labels}')\n",
    "    \n",
    "    val_loss = running_loss / processed_size\n",
    "    # val_MSE = running_MSE / processed_size\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, optimizer, scheduler=None, epochs=10, scaler=None, criterion=nn.MSELoss()):\n",
    "    '''\n",
    "\n",
    "    Basic option: calculation loss on K-dimensional outputs, but MSE metric on 3D outputs, after the matrix is applied\n",
    "\n",
    "    loss_on_k_projections: calculate loss'\n",
    "    \n",
    "    '''\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} val_loss {v_loss:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = fit_epoch(model, train_loader, criterion, optimizer, scheduler, scaler)\n",
    "\n",
    "            val_loss = eval_epoch(model, val_loader, criterion)\n",
    "            \n",
    "            history.append((train_loss, val_loss))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch + 1, t_loss=train_loss, v_loss=val_loss))\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleNet(nn.Module):\n",
    "    '''\n",
    "\n",
    "    Класс одиночной нейронной сети\n",
    "\n",
    "    '''\n",
    "    def __init__(self, output_size, activation=nn.ReLU(), flattened_size=CFG.K * CFG.K):\n",
    "        '''\n",
    "        \n",
    "        FC_type: тип полносвязных слоев: 'regular' / 'simple\n",
    "\n",
    "        convolution: сверточная часть сети\n",
    "\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.FC = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 64),\n",
    "            activation,\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(64),\n",
    "\n",
    "            nn.Linear(64, 128),\n",
    "            activation,\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(128),\n",
    "            \n",
    "            nn.Linear(128, 256),\n",
    "            activation,\n",
    "            # nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, output_size),\n",
    "        )\n",
    "\n",
    "        # self.FC = nn.Linear(flattened_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x - is batch of matrices KxK\n",
    "\n",
    "        # Здесь происходят какие-то там свертки, пуллинги и тп..\n",
    "\n",
    "        x = self.FC(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recieve_loaders(take_one_projection_for_data=None, path=None, cut_size=None):\n",
    "    '''\n",
    "    returns: (dataet, train_loader, val_loader), if u pass path -> returns loader from tensor dataset from\n",
    "\n",
    "    может создать датасет у которого всего одна проекция взята за таргет из уже существующего датасета или по пути на файл:\n",
    "    take_one_projection_for_data - указываем в этом параметре номер координатной проекции (нумерация от 1)\n",
    "    '''\n",
    "    if path:\n",
    "        N = int(path.split('_')[0])\n",
    "        K = int(path.split('_')[-1].split('.')[0])\n",
    "\n",
    "        dataset = torch.load(str(N) + '_dataset_K_' + str(K) + '.pt')\n",
    "        dataset = [(elem[0], elem[1]) for elem in dataset]\n",
    "\n",
    "        if take_one_projection_for_data:\n",
    " \n",
    "            dataset = [(elem[0], elem[-1][take_one_projection_for_data - 1].unsqueeze(dim=0)) for elem in dataset]\n",
    "\n",
    "        train_data, val_data = train_test_split(dataset, test_size=0.33, random_state=42)\n",
    "        if cut_size:\n",
    "            train_data = train_data[:cut_size]\n",
    "            val_data = val_data[:cut_size]\n",
    "        train_dataloader, val_dataloader = create_dataloaders(train_data, val_data)\n",
    "        return train_data, val_data, train_dataloader, val_dataloader\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for _ in range(500):\n",
    "        # X = (torch.rand(1)).squeeze().unsqueeze(dim=0)\n",
    "        x = (torch.rand(K))\n",
    "        y = function(x)\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, random_state=42, train_size=0.8)\n",
    "\n",
    "    X_train = torch.stack(X_train)\n",
    "    X_val = torch.stack(X_val)\n",
    "    Y_train = torch.stack(Y_train)\n",
    "    Y_val = torch.stack(Y_val)\n",
    "\n",
    "    train_data = TensorDataset(X_train, Y_train)\n",
    "    val_data = TensorDataset(X_val, Y_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=128)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=128)\n",
    "\n",
    "    return train_data, val_data, train_dataloader, val_dataloader\n",
    "    \n",
    "train_data, val_data, train_dataloader, val_dataloader = recieve_loaders(take_one_projection_for_data=1, path=path, cut_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сама частица, отступаем от нее вектор силы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N=2, K=1:\n",
    "\n",
    "# flattened_size = CFG.K ** 2\n",
    "# output_size = CFG.K\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(flattened_size, 8),\n",
    "#     nn.ReLU(),\n",
    "#     # nn.Dropout(0.3),\n",
    "#     nn.BatchNorm1d(8),\n",
    "\n",
    "#     nn.Linear(8, 32),\n",
    "#     nn.ReLU(),\n",
    "#     # nn.Dropout(0.3),\n",
    "#     nn.BatchNorm1d(32),\n",
    "\n",
    "#     nn.Linear(32, 128),\n",
    "#     nn.ReLU(),\n",
    "#     # nn.Dropout(0.3),\n",
    "#     nn.BatchNorm1d(128),\n",
    "#     nn.Linear(128, output_size),\n",
    "#     ).to(CFG.device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=5e-3, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "# exp_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleNet(\n",
       "  (FC): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SingleNet(\n",
    "    output_size=1,\n",
    "\n",
    "    # activation=nn.Sigmoid(),\n",
    "    activation=nn.ReLU(),\n",
    "\n",
    ").to(CFG.device)\n",
    "\n",
    "# model = nn.Linear(1, 1)\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(CFG.K ** 2, 8),\n",
    "#     nn.ReLU(),\n",
    "#     # nn.Dropout(0.5),\n",
    "#     # nn.BatchNorm1d(8),\n",
    "\n",
    "#     nn.Linear(8, 32),\n",
    "#     nn.ReLU(),\n",
    "#     # nn.Dropout(0.5),\n",
    "#     # nn.BatchNorm1d(32),\n",
    "\n",
    "#     nn.Linear(32, CFG.K)\n",
    "# )\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "# scheduler.step нужно первый раз делать обязательно после optimizer.step, потому что иначе мы просто пропустим первый шаг scheduler\n",
    "exp_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.95)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  32%|███▏      | 16/50 [00:00<00:00, 102.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 2.4729 val_loss 1.0686\n",
      "\n",
      "Epoch 002 train_loss: 1.5410 val_loss 1.0186\n",
      "\n",
      "Epoch 003 train_loss: 1.0262 val_loss 0.9718\n",
      "\n",
      "Epoch 004 train_loss: 0.8049 val_loss 0.9150\n",
      "\n",
      "Epoch 005 train_loss: 0.6770 val_loss 0.8558\n",
      "\n",
      "Epoch 006 train_loss: 0.5576 val_loss 0.8199\n",
      "\n",
      "Epoch 007 train_loss: 0.5582 val_loss 0.8064\n",
      "\n",
      "Epoch 008 train_loss: 0.4875 val_loss 0.7975\n",
      "\n",
      "Epoch 009 train_loss: 0.4120 val_loss 0.7720\n",
      "\n",
      "Epoch 010 train_loss: 0.3604 val_loss 0.7495\n",
      "\n",
      "Epoch 011 train_loss: 0.2937 val_loss 0.7351\n",
      "\n",
      "Epoch 012 train_loss: 0.2911 val_loss 0.7316\n",
      "\n",
      "Epoch 013 train_loss: 0.2572 val_loss 0.7402\n",
      "\n",
      "Epoch 014 train_loss: 0.2759 val_loss 0.7470\n",
      "\n",
      "Epoch 015 train_loss: 0.2553 val_loss 0.7466\n",
      "\n",
      "Epoch 016 train_loss: 0.2465 val_loss 0.7507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  78%|███████▊  | 39/50 [00:00<00:00, 95.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 017 train_loss: 0.2772 val_loss 0.7556\n",
      "\n",
      "Epoch 018 train_loss: 0.2029 val_loss 0.7673\n",
      "\n",
      "Epoch 019 train_loss: 0.1885 val_loss 0.7854\n",
      "\n",
      "Epoch 020 train_loss: 0.1845 val_loss 0.7992\n",
      "\n",
      "Epoch 021 train_loss: 0.2064 val_loss 0.8041\n",
      "\n",
      "Epoch 022 train_loss: 0.1909 val_loss 0.8098\n",
      "\n",
      "Epoch 023 train_loss: 0.1631 val_loss 0.8346\n",
      "\n",
      "Epoch 024 train_loss: 0.1603 val_loss 0.8410\n",
      "\n",
      "Epoch 025 train_loss: 0.1471 val_loss 0.8494\n",
      "\n",
      "Epoch 026 train_loss: 0.1751 val_loss 0.8505\n",
      "\n",
      "Epoch 027 train_loss: 0.1394 val_loss 0.8517\n",
      "\n",
      "Epoch 028 train_loss: 0.1689 val_loss 0.8654\n",
      "\n",
      "Epoch 029 train_loss: 0.1796 val_loss 0.8860\n",
      "\n",
      "Epoch 030 train_loss: 0.1416 val_loss 0.9048\n",
      "\n",
      "Epoch 031 train_loss: 0.1364 val_loss 0.9129\n",
      "\n",
      "Epoch 032 train_loss: 0.1317 val_loss 0.9187\n",
      "\n",
      "Epoch 033 train_loss: 0.1332 val_loss 0.9401\n",
      "\n",
      "Epoch 034 train_loss: 0.1168 val_loss 0.9676\n",
      "\n",
      "Epoch 035 train_loss: 0.2215 val_loss 0.9984\n",
      "\n",
      "Epoch 036 train_loss: 0.1295 val_loss 1.0412\n",
      "\n",
      "Epoch 037 train_loss: 0.1646 val_loss 1.0389\n",
      "\n",
      "Epoch 038 train_loss: 0.1607 val_loss 0.9670\n",
      "\n",
      "Epoch 039 train_loss: 0.1182 val_loss 0.9549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 50/50 [00:00<00:00, 98.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 040 train_loss: 0.1118 val_loss 1.0051\n",
      "\n",
      "Epoch 041 train_loss: 0.1096 val_loss 1.0693\n",
      "\n",
      "Epoch 042 train_loss: 0.0936 val_loss 1.0848\n",
      "\n",
      "Epoch 043 train_loss: 0.0872 val_loss 1.0443\n",
      "\n",
      "Epoch 044 train_loss: 0.0977 val_loss 0.9955\n",
      "\n",
      "Epoch 045 train_loss: 0.0722 val_loss 0.9719\n",
      "\n",
      "Epoch 046 train_loss: 0.1057 val_loss 0.9995\n",
      "\n",
      "Epoch 047 train_loss: 0.1470 val_loss 1.0372\n",
      "\n",
      "Epoch 048 train_loss: 0.1532 val_loss 1.0572\n",
      "\n",
      "Epoch 049 train_loss: 0.0855 val_loss 1.0230\n",
      "\n",
      "Epoch 050 train_loss: 0.1053 val_loss 1.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = train(\n",
    "    train_loader=train_dataloader, val_loader=val_dataloader, model=model, optimizer=optimizer,\n",
    "    \n",
    "    scheduler=exp_scheduler,\n",
    "\n",
    "    scaler=None,\n",
    "    \n",
    "    # criterion=MultiOutputMSELoss(),     # on K=1 it's the same as nn.MSELoss\n",
    "    criterion=nn.MSELoss(),\n",
    "    # criterion = GaussianNLLLossWithReadyVar(),\n",
    "    \n",
    "    epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: 0.10423701256513596\n"
     ]
    }
   ],
   "source": [
    "make_predictions_and_plot(model=model, data=train_data, same_axis=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('my_3_6_conda_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b5cc6bf18c7103cb99324f044582e9ca68eda52a25f7227b20ca62cd3e32898"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
